{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import operations as op\n",
    "import layers\n",
    "from tensorflow.keras.layers import Conv1D,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import douban_evaluation as eva\n",
    "from douban_evaluation import evaluation_one_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0355096, 0), (-4.97581, 0), (-1.66428, 0), (4.21394, 0), (3.91111, 0), (0.78502, 0), (2.30278, 0), (-1.16753, 0), (-0.440959, 0), (3.17839, 0)]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e84110041f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mtotal_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mm_a_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_r_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_one_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0msum_m_a_p\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mm_a_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0msum_m_r_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mm_r_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfsdata2/dongxz1_data/research/Dialogue/DAM/utils/douban_evaluation.py\u001b[0m in \u001b[0;36mevaluation_one_session\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msort_lable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms_d\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mselect_lable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_lable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mselect_lable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msort_lable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluation_one_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfsdata2/dongxz1_data/research/Dialogue/DAM/utils/douban_evaluation.py\u001b[0m in \u001b[0;36mmean_average_precision\u001b[0;34m(sort_data)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcount_1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0msum_precision\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcount_1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount_1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m          \u001b[0;32mreturn\u001b[0m \u001b[0msum_precision\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount_1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "sum_m_a_p = 0\n",
    "sum_m_r_r = 0\n",
    "sum_p_1 = 0\n",
    "sum_r_1 = 0\n",
    "sum_r_2 = 0\n",
    "sum_r_5 = 0\n",
    "\n",
    "i = 0\n",
    "total_num = 0\n",
    "with open('../output/douban/DAM_hireaction/score.test', 'r') as infile:\n",
    "        for line in infile:\n",
    "            if i % 10 == 0:\n",
    "                data = []\n",
    "            \n",
    "            tokens = line.strip().split('\\t')\n",
    "            data.append((float(tokens[0]), int(tokens[1])))\n",
    "            if i % 10 == 9:\n",
    "                print(data)\n",
    "                total_num += 1\n",
    "                m_a_p, m_r_r, p_1, r_1, r_2, r_5 = evaluation_one_session(data)\n",
    "                sum_m_a_p += m_a_p\n",
    "                sum_m_r_r += m_r_r\n",
    "                sum_p_1 += p_1\n",
    "                sum_r_1 += r_1\n",
    "                sum_r_2 += r_2\n",
    "                sum_r_5 += r_5\n",
    "\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x  = tf.placeholder(tf.float32,shape=[10,20,100])\n",
    "input_y = tf.placeholder(tf.float32,shape=[10,20,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable positional_encoding_one/lambda already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/dfsdata2/dongxz1_data/research/Dialogue/DAM/utils/operations.py\", line 307, in positional_encoding_vector\n    initializer=tf.constant_initializer(value))\n  File \"<ipython-input-3-51fd59dbef65>\", line 2, in <module>\n    input_x = op.positional_encoding_vector(input_x, max_timescale=10)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-51fd59dbef65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positional_encoding_one'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positional_encoding_two'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfsdata2/dongxz1_data/research/Dialogue/DAM/utils/operations.py\u001b[0m in \u001b[0;36mpositional_encoding_vector\u001b[0;34m(x, min_timescale, max_timescale, value)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         initializer=tf.constant_initializer(value))\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0m_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1465\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1215\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    525\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    479\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 848\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable positional_encoding_one/lambda already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/dfsdata2/dongxz1_data/research/Dialogue/DAM/utils/operations.py\", line 307, in positional_encoding_vector\n    initializer=tf.constant_initializer(value))\n  File \"<ipython-input-3-51fd59dbef65>\", line 2, in <module>\n    input_x = op.positional_encoding_vector(input_x, max_timescale=10)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('positional_encoding_one'):\n",
    "    input_x = op.positional_encoding_vector(input_x, max_timescale=10)\n",
    "with tf.variable_scope('positional_encoding_two'):\n",
    "    input_y = op.positional_encoding_vector(input_y, max_timescale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_max_pooling1d_1/Max:0' shape=(10, 128) dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Conv1D(filters=128, kernel_size=5, activation='relu')(input_x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention(\n",
    "    Q, K, V, \n",
    "    Q_lengths, K_lengths, \n",
    "    attention_type='dot', \n",
    "    is_mask=True, mask_value=-2**32+1,\n",
    "    drop_prob=None):\n",
    "    '''Add attention layer.\n",
    "    Args:\n",
    "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
    "        K: a tensor with shape [batch, time, K_dimension]\n",
    "        V: a tensor with shape [batch, time, V_dimension]\n",
    "\n",
    "        Q_length: a tensor with shape [batch]\n",
    "        K_length: a tensor with shape [batch]\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, Q_time, V_dimension]\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: if\n",
    "            Q_dimension not equal to K_dimension when attention type is dot.\n",
    "    '''\n",
    "    assert attention_type in ('dot', 'bilinear')\n",
    "    if attention_type == 'dot':\n",
    "        assert Q.shape[-1] == K.shape[-1]\n",
    "\n",
    "    Q_time = Q.shape[1]\n",
    "    K_time = K.shape[1]\n",
    "\n",
    "    if attention_type == 'dot':\n",
    "        logits = op.dot_sim(Q, K) #[batch, Q_time, time]\n",
    "    if attention_type == 'bilinear':\n",
    "        logits = op.bilinear_sim(Q, K)\n",
    "\n",
    "    if is_mask:\n",
    "        mask = op.mask(Q_lengths, K_lengths, Q_time, K_time) #[batch, Q_time, K_time]\n",
    "        logits = mask * logits + (1 - mask) * mask_value\n",
    "    \n",
    "    attention = tf.nn.softmax(logits)\n",
    "\n",
    "    if drop_prob is not None:\n",
    "        print('use attention drop')\n",
    "        attention = tf.nn.dropout(attention, drop_prob)\n",
    "\n",
    "    return op.weighted_sum(attention, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    '''attention(Q, K, V) = softmax(Q * K^T / sqrt(dk)) * V'''\n",
    "    # query 和 Key相乘\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    # 使用dk进行缩放\n",
    "    dk = tf.cast(tf.shape(q)[-1], tf.float32)\n",
    "    scaled_attention =matmul_qk / tf.sqrt(dk)\n",
    "    # 掩码mask\n",
    "    if mask is not None:\n",
    "        # 这里将mask的token乘以-1e-9，这样与attention相加后，mask的位置经过softmax后就为0\n",
    "        # padding位置 mask=1\n",
    "        scaled_attention += mask * -1e-9\n",
    "    # 通过softmax获取attention权重, mask部分softmax后为0\n",
    "    attention_weights = tf.nn.softmax(scaled_attention)  # shape=[batch_size, seq_len_q, seq_len_k]\n",
    "    # 乘以value\n",
    "    outputs = tf.matmul(attention_weights, v)  # shape=[batch_size, seq_len_q, depth]\n",
    "    return outputs, attention_weights\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        # d_model必须可以正确分成多个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 分头之后维度\n",
    "        self.depth = d_model // num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 分头，将头个数的维度，放到seq_len前面 x输入shape=[batch_size, seq_len, d_model]\n",
    "        x = tf.reshape(x, [batch_size, -1, self.num_heads, self.depth])\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        # 分头前的前向网络，根据q,k,v的输入，计算Q, K, V语义\n",
    "        q = self.wq(q)  # shape=[batch_size, seq_len_q, d_model]\n",
    "        k = self.wq(k)\n",
    "        v = self.wq(v)\n",
    "        # 分头\n",
    "        q = self.split_heads(q, batch_size)  # shape=[batch_size, num_heads, seq_len_q, depth]\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # 通过缩放点积注意力层\n",
    "        # scaled_attention shape=[batch_size, num_heads, seq_len_q, depth]\n",
    "        # attention_weights shape=[batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # shape=[batch_size, seq_len_q, num_heads, depth]\n",
    "        # 把多头合并\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # shape=[batch_size, seq_len_q, d_model]\n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "turns_len = 5\n",
    "words = 10\n",
    "dim =10\n",
    "\n",
    "input_turns = tf.placeholder(tf.float32, [2,5,10,10])\n",
    "respones  = tf.placeholder(tf.float32, [batch_size, words, dim])\n",
    "respones_len = tf.placeholder(tf.int32,[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(2, 5, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input_turns = tf.transpose(input_turns,perm=[1,0,2,3])\n",
    "# input_turns = tf.transpose(input_turns,perm=[1,0,2,3])\n",
    "print(input_turns)\n",
    "_turn_match = []\n",
    "\n",
    "for _t in tf.split(input_turns,5,1):\n",
    "    _t = tf.squeeze(_t)\n",
    "    _match_result= attention(respones, _t,  _t, respones_len, respones_len)\n",
    "    _turn_match.append(tf.expand_dims(_match_result,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_11:0' shape=(2, 5, 10, 10) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_turn_match = tf.concat(_turn_match,1)\n",
    "best_turn_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = MultiHeadAttention(dim,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FFN(x, out_dimension_0=None, out_dimension_1=None):\n",
    "    '''Add two dense connected layer, max(0, x*W0+b0)*W1+b1.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time, dimension]\n",
    "        out_dimension: a number which is the output dimension\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, time, out_dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    with tf.variable_scope('FFN_1'):\n",
    "        y = op.dense(x, out_dimension_0)\n",
    "        y = tf.nn.relu(y)\n",
    "    with tf.variable_scope('FFN_2'):\n",
    "        z = op.dense(y, out_dimension_1) #, add_bias=False)  #!!!!\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'multi_head_attention_1/dense_3/BiasAdd:0' shape=(2, 10, 10) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,_ = multihead(respones, best_turn_match, best_turn_match)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = FFN(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'FFN_2/add:0' shape=(2, 10, 10) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
